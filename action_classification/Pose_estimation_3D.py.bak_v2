
# Copyright (c) OpenMMLab. All rights reserved.
import os
import mmcv
import warnings
import pickle
import copy
import numpy as np
from mmpose.apis import (inference_top_down_pose_model,inference_pose_lifter_model, init_pose_model,
                         process_mmdet_results, vis_3d_pose_result,extract_pose_sequence)
from mmpose.datasets import DatasetInfo
try:
    from mmdet.apis import inference_detector, init_detector
    has_mmdet = True
except (ImportError, ModuleNotFoundError):
    has_mmdet = False
import cv2
import os.path as osp
#from mmpose.apis.inference import init_pose_model

class Pose_3D_estimation():

    def __init__(self):
        self.pose_lifter_config = "action_classification/configs/body/3d_kpt_sview_rgb_vid/video_pose_lift/h36m/videopose3d_h36m_243frames_fullconv_supervised_cpn_ft.py"
        self.pose_lifter_checkpoint = "action_classification/checkpoints/videopose_h36m_243frames_fullconv_supervised_cpn_ft-88f5abbb_20210527.pth"
        #self.pose_config_2D = f'action_classification/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w32_coco_256x192.py'  # noqa: E501
        self.pose_config_2D = f'action_classification/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'
        #self.pose_checkpoint_2D = 'action_classification/checkpoints/hrnet_w32_coco_256x192-c78dce93_20200708.pth'  # noqa: E501
        self.pose_checkpoint_2D = 'action_classification/checkpoints/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'  # noqa: E501
        self.device = "cuda:0"
        self.det_config = f'action_classification/configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco-person.py'  # noqa: E501
        # args.det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth'  # noqa: E501
        self.det_checkpoint = 'action_classification/checkpoints/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth'  # noqa: E501
        self.det_score_thr = 0.5
        self.det_cat_id = 1
        self.bbox_thr = 0.9
        self.kpt_thr = 0.3
        self.radius = 8
        self.thickness = 2
        self.out_img_root = None
        assert has_mmdet, 'Please install mmdet to run the demo.'


    def init_top_down_model(self):
        # 初始化模型
        assert self.det_config is not None
        assert self.det_checkpoint is not None
        #初始化人体检测模型
        self.det_model = init_detector(
            self.det_config, self.det_checkpoint, device=self.device.lower())
        # 初始化2d姿态估计模型
        self.pose_model_2D = init_pose_model(
            self.pose_config_2D, self.pose_checkpoint_2D, device=self.device.lower())
        self.dataset_2d = self.pose_model_2D.cfg.data['test']['type']
        self.dataset_info_2d = self.pose_model_2D.cfg.data['test'].get('dataset_info', None)
        if self.dataset_info_2d is None:
            warnings.warn(
                'Please set `dataset_info` in the config.'
                'Check https://github.com/open-mmlab/mmpose/pull/663 for details.',
                DeprecationWarning)
        else:
            self.dataset_info_2d = DatasetInfo(self.dataset_info_2d)

        #初始化3D姿态估计模型
        self.pose_lift_model = init_pose_model(
            self.pose_lifter_config,
            self.pose_lifter_checkpoint,
            device=self.device.lower())
        assert self.pose_lift_model.cfg.model.type == 'PoseLifter', 'Only' \
                                                               '"PoseLifter" model is supported for the 2nd stage ' \
                                                               '(2D-to-3D lifting)'
        self.dataset_3D = self.pose_lift_model.cfg.data['test']['type']
        self.dataset_info_3D = self.pose_lift_model.cfg.data['test'].get('dataset_info', None)
        if self.dataset_info_3D is None:
            warnings.warn(
                'Please set `dataset_info` in the config.'
                'Check https://github.com/open-mmlab/mmpose/pull/663 for details.',
                DeprecationWarning)
        else:
            self.dataset_info_3D = DatasetInfo(self.dataset_info_3D)



    def process_img(self,image, is_show_keypoints=False):
        """
        Args:
            image: cv2 img or path str
        Returns:
        """
        if isinstance(image,str):
            image_name = image.split("/")[-1].split(".")[0]
            image = cv2.imread(image)
            self.out_img_root = "result_img"

        # test a single image, the resulting box is (x1, y1, x2, y2)
        mmdet_results = inference_detector(self.det_model, image)
        # keep the person class bounding boxes.
        person_results = process_mmdet_results(mmdet_results, self.det_cat_id)

        pose_det_results_list = []
        next_id = 0
        pose_det_results = []

        pose_det_results, returned_outputs = inference_top_down_pose_model(
            self.pose_model_2D,
            image,
            person_results,
            bbox_thr=self.bbox_thr,
            format='xyxy',
            dataset=self.dataset_2d,
            dataset_info=self.dataset_info_2d,
            return_heatmap=False,
            outputs=None)
        pose_det_results_list.append(copy.deepcopy(pose_det_results))

        for res in pose_det_results:
            keypoints = res['keypoints']
            res['keypoints'] = self.covert_keypoint_definition(
                keypoints, self.dataset_2d, self.dataset_3D)

        if hasattr(self.pose_lift_model.cfg, 'test_data_cfg'):
            data_cfg = self.pose_lift_model.cfg.test_data_cfg
        else:
            data_cfg = self.pose_lift_model.cfg.data_cfg    # ----------------加到初始化

        num_instances = -1
        for i, pose_det_results in enumerate(
                mmcv.track_iter_progress(pose_det_results_list)):
            # extract and pad input pose2d sequence
            pose_results_2d = extract_pose_sequence(
                pose_det_results_list,
                frame_idx=i,
                causal=data_cfg.causal,
                seq_len=data_cfg.seq_len,
                step=data_cfg.seq_frame_interval)
            # 2D-to-3D pose lifting

            pose_lift_results = inference_pose_lifter_model(
                self.pose_lift_model,
                pose_results_2d=pose_results_2d,
                dataset=self.dataset_3D,
                dataset_info=self.dataset_info_3D,
                with_track_id=False,
                image_size=image.shape[0:2],
                norm_pose_2d=True)

            # Pose processing
            pose_lift_results_vis = []
            for idx, res in enumerate(pose_lift_results):
                keypoints_3d = res['keypoints_3d']
                # exchange y,z-axis, and then reverse the direction of x,z-axis
                keypoints_3d = keypoints_3d[..., [0, 2, 1]]
                keypoints_3d[..., 0] = -keypoints_3d[..., 0]
                keypoints_3d[..., 2] = -keypoints_3d[..., 2]
                # rebase height (z-axis)
                rebase_keypoint_height = True
                if rebase_keypoint_height:
                    keypoints_3d[..., 2] -= np.min(
                        keypoints_3d[..., 2], axis=-1, keepdims=True)
                res['keypoints_3d'] = keypoints_3d
                # add title
                det_res = pose_det_results[idx]
                res['title'] = f'Prediction ()'
                # only visualize the target frame
                res['keypoints'] = det_res['keypoints']
                res['bbox'] = det_res['bbox']
                pose_lift_results_vis.append(res)

            # Visualization
            if num_instances < 0:
                num_instances = len(pose_lift_results_vis)
            img_vis = None
            if is_show_keypoints:
                # img_vis = vis_3d_pose_result(
                #     self.pose_lift_model,
                #     result=pose_lift_results_vis,
                #     img=image,
                #     out_file=None,
                #     radius=8,
                #     thickness=2,
                #     num_instances=num_instances)
                img_vis = vis_3d_pose_result(
                    self.pose_lift_model,
                    result=pose_lift_results_vis,
                    img=image,
                    dataset_info=self.dataset_info_3D,
                    out_file=None)

        # 增加返回人体检测框 mmdet_results 用来绑定其他信息
        return img_vis, pose_lift_results, mmdet_results

    def covert_keypoint_definition(self,keypoints, pose_det_dataset, pose_lift_dataset):
        """Convert pose det dataset keypoints definition to pose lifter dataset
        keypoints definition.

        Args:
            keypoints (ndarray[K, 2 or 3]): 2D keypoints to be transformed.
            pose_det_dataset, (str): Name of the dataset for 2D pose detector.
            pose_lift_dataset (str): Name of the dataset for pose lifter model.
        """
        if pose_det_dataset == 'TopDownH36MDataset' and \
                pose_lift_dataset == 'Body3DH36MDataset':
            return keypoints
        elif pose_det_dataset == 'TopDownCocoDataset' and \
                pose_lift_dataset == 'Body3DH36MDataset':
            keypoints_new = np.zeros((17, keypoints.shape[1]))
            # pelvis is in the middle of l_hip and r_hip
            keypoints_new[0] = (keypoints[11] + keypoints[12]) / 2
            # thorax is in the middle of l_shoulder and r_shoulder
            keypoints_new[8] = (keypoints[5] + keypoints[6]) / 2
            # head is in the middle of l_eye and r_eye
            keypoints_new[10] = (keypoints[1] + keypoints[2]) / 2
            # spine is in the middle of thorax and pelvis
            keypoints_new[7] = (keypoints_new[0] + keypoints_new[8]) / 2
            # rearrange other keypoints
            keypoints_new[[1, 2, 3, 4, 5, 6, 9, 11, 12, 13, 14, 15, 16]] = \
                keypoints[[12, 14, 16, 11, 13, 15, 0, 5, 7, 9, 6, 8, 10]]
            return keypoints_new
        else:
            raise NotImplementedError
#1.获取文件名列表
def get_file_names(folder_dir):
    file_names = os.listdir(folder_dir)
    file_names = [osp.join(folder_dir, vid_n) for vid_n in file_names]
    return file_names

def extract_pose_from_video(is_save_img=False):
    """
    从视频中提取骨骼数据，并将结果保存为图片
    Returns:
    """
    det_model,pose_model = init_top_down_model()
    file_names = get_file_names(r"/content/mmpose/data/train")
    train_data = []
    for file_name in file_names:
        label = int(file_name.split('/')[-1].split('A')[1][:3])
        cap = cv2.VideoCapture(file_name)
        total_frame = 0
        video_poses = []
        img_num = 0
        img_save_dir = file_name.split('.')[0]
        if not os.path.exists(img_save_dir) and is_save_img:
            os.mkdir(img_save_dir)
        while True:
            ret,frame = cap.read()
            if not ret:
                break
            img, pose_results = top_down_img(frame,det_model,pose_model)
            total_frame += 1
            if len(pose_results) == 1 and total_frame%3 == 0:  #每3帧保存一次
                img_num += 1
                if is_save_img:
                    img_save_name = os.path.join(img_save_dir, str(img_num) + '.jpg')
                    cv2.imwrite(img_save_name, img)
                    print("save image to {}".format(img_save_name))
                video_poses.append(pose_results[0]["keypoints"])  # 0 每一帧只有一个人被识别
        train_data.append(dict(label=label,video_name=file_name,keypoints=video_poses))
        print("video:{}  process finished".format(file_name))
    with open("/content/mmpose/data/train/train.pkl", "wb") as fo:
        pickle.dump(train_data,fo)
    print("all video done!")

if __name__ == '__main__':
    file_name = "../123.jpg"
    estimationer = Pose_3D_estimation()
    estimationer.init_top_down_model()
    estimationer.process_img(file_name)


